import time

from llamaapi import LlamaAPI
from project.utils.config import Config
import logging
from project.utils.singleton_meta import SingletonMeta

logger = logging.getLogger(__name__)


class LlamaClient:
    """
    Client to interact with the Llama API.

    Attributes:
        config (Config): Configuration object to fetch API keys and settings.
        client (OpenAI): OpenAI client initialized with the API key.
        system_context (str): System content to guide the OpenAI model.
    """

    def __init__(self) -> None:
        """
        Initializes the LlamaClient with the necessary configurations.
        """
        self.config = Config()
        self.client = LlamaAPI(self.config.get_llama_key())
        self.system_context = self.config.get_llm_system_context()
        logger.info("LlamaClient initialized")

    def _generate_response(self, prompt: str, placeholder: str, text: str, model: str) -> tuple[str, float]:
        """
        Generates a response using the Llama API by replacing a placeholder in the prompt with the given text.

        Args:
            prompt (str): The base prompt template.
            placeholder (str): The placeholder text to be replaced.
            text (str): The text to insert into the placeholder.
            model (str): The model to use for generating the response.

        Returns:
            str: The generated response.
        """
        final_prompt = prompt.replace(placeholder, text)
        logger.info(f"Generating text with prompt: {final_prompt}")

        try:
            start_time = time.time()
            api_request_json = {
                "model": model,
                "messages": [
                    {"role": "system", "content": self.system_context},
                    {"role": "user", "content": final_prompt}
                ]
            }
            response = self.client.run(api_request_json)
            end_time = time.time()
            elapsed_time = end_time - start_time
            return response.json(), elapsed_time
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return "An error occurred while generating the response.", 0

    def analyze_tweets(self, review: str) -> tuple[str, float]:
        """
        Analyzes the given tweet review by generating a response from the Llama API.

        Args:
            review (str): The tweet review to be analyzed.

        Returns:
            str: The response generated by the Llama model.
        """
        return self._generate_response(
            self.config.get_llm_prompt("tweet"), "{text_tweet}", review, self.config.get_llama_llm()
        )

    def analyze_news(self, review: str) -> tuple[str, float]:
        """
        Analyzes the given news review by generating a response from the Llama API.

        Args:
            review (str): The news review to be analyzed.

        Returns:
            str: The response generated by the Llama model.
        """
        return self._generate_response(
            self.config.get_llm_prompt("notice"), "{text_new}", review, self.config.get_llama_llm()
        )

    def verify_api_key(self) -> bool:
        """
        Verifies if the provided API key for the Llama API is valid.

        Returns:
            bool: True if the API key is valid, False otherwise.
        """
        try:
            self.client
        except Exception as e:
            logger.error(f"API key verification failed: {e}")
            return False
        else:
            return True
